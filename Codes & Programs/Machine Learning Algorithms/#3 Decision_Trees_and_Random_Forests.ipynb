{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1zP9pqjrGQ64"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation"
      ],
      "metadata": {
        "id": "dlCbcBsvGH4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]])  # Features\n",
        "y = np.array([0, 1, 1, 0, 0, 0])  # Labels\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "z6GxMCz1GSvr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree\n",
        "In a Decision Tree, the topmost node is known as the root node. It works by partitioning the dataset into subsets based on the feature that provides the best separation between the target variable classes. The partitioning process is done recursively, producing a tree with decision nodes and leaf nodes. A decision node has two or more branches, and a leaf node represents a classification or decision. The decision tree makes decisions by asking multiple questions and following the path down the tree that corresponds to the answer."
      ],
      "metadata": {
        "id": "irHbznYPGhI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    def entropy(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "    def information_gain(self, X, y, feature_index):\n",
        "        original_entropy = self.entropy(y)\n",
        "        values = np.unique(X[:, feature_index])\n",
        "        weighted_entropy = 0\n",
        "        for value in values:\n",
        "            subset = y[X[:, feature_index] == value]\n",
        "            weighted_entropy += (len(subset) / len(y)) * self.entropy(subset)\n",
        "        return original_entropy - weighted_entropy\n",
        "\n",
        "    def build_tree(self, X, y, features):\n",
        "        unique_classes = np.unique(y)\n",
        "        if len(unique_classes) == 1 or len(features) == 0:\n",
        "            return unique_classes[0]\n",
        "        gains = [self.information_gain(X, y, feature) for feature in features]\n",
        "        best_feature = features[np.argmax(gains)]\n",
        "        features = [f for f in features if f != best_feature]\n",
        "        tree = {}\n",
        "        for value in np.unique(X[:, best_feature]):\n",
        "            subset_X = X[X[:, best_feature] == value]\n",
        "            subset_y = y[X[:, best_feature] == value]\n",
        "            tree[f\"{best_feature} = {value}\"] = self.build_tree(subset_X, subset_y, features)\n",
        "        return tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        features = list(range(X.shape[1]))\n",
        "        self.tree = self.build_tree(X, y, features)\n",
        "\n",
        "    def predict_single(self, tree, sample):\n",
        "        if not isinstance(tree, dict):\n",
        "            return tree\n",
        "        for node, subtree in tree.items():\n",
        "            feature, value = node.split(\" = \")\n",
        "            feature = int(feature)\n",
        "            if sample[feature] == int(value):\n",
        "                return self.predict_single(subtree, sample)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.predict_single(self.tree, sample) for sample in X]"
      ],
      "metadata": {
        "id": "4uZbUA9wGjjg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest\n",
        "A Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It's primarily used for classification tasks but can also be applied to regression problems. The idea is to build multiple decision trees during training and output the mode of the classes (classification) or mean prediction (regression) of the individual trees for unseen data."
      ],
      "metadata": {
        "id": "_uY6aYzBG1R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_trees, sample_size, feature_size):\n",
        "        self.n_trees = n_trees\n",
        "        self.sample_size = sample_size\n",
        "        self.feature_size = feature_size\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_trees):\n",
        "            indices = np.random.choice(len(X), size=self.sample_size, replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            features = np.random.choice(X.shape[1], size=self.feature_size, replace=False)\n",
        "            tree = DecisionTree()\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return [Counter(predictions[:, i]).most_common(1)[0][0] for i in range(len(X))]"
      ],
      "metadata": {
        "id": "L2A5eBToG52o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation"
      ],
      "metadata": {
        "id": "IC7OCjWYHBgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "dt = DecisionTree()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_predictions = dt.predict(X_test)\n",
        "print(\"Decision Tree Predictions:\", dt_predictions)\n",
        "print(\"Decision Tree Accuracy:\", np.mean(dt_predictions == y_test))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForest(n_trees=3, sample_size=3, feature_size=1)  # Adjusted hyperparameters\n",
        "rf.fit(X_train, y_train)\n",
        "rf_predictions = rf.predict(X_test)\n",
        "print(\"Random Forest Predictions:\", rf_predictions)\n",
        "print(\"Random Forest Accuracy:\", np.mean(rf_predictions == y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onq7egX0HDGn",
        "outputId": "f846ff3f-1108-480b-a329-a1355c069173"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Predictions: [0, 0]\n",
            "Decision Tree Accuracy: 0.5\n",
            "Random Forest Predictions: [0, 0]\n",
            "Random Forest Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}