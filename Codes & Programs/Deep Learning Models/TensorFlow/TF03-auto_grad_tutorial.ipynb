{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic Differentiation and Gradients\n",
        "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks."
      ],
      "metadata": {
        "id": "17dY6KYtfBN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SVR_At_3cp2I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Tapes"
      ],
      "metadata": {
        "id": "m7P5qL0US0Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with a sclars\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x**2\n",
        "\n",
        "# Calculate the gradient\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leYRCtGvS2E7",
        "outputId": "325b10c8-4cfd-4903-c805-eaa2ded66977"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with a tensor\n",
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    y = x @ w + b\n",
        "    loss = tf.reduce_mean(y**2)\n",
        "\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
        "\n",
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhGa4spbTXh9",
        "outputId": "04ce0c41-ef9b-447f-91b6-ddf4ab568b99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with a simple model\n",
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Forward pass\n",
        "    y = layer(x)\n",
        "    loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)\n",
        "\n",
        "# Print\n",
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "    print(f'{var.name}, shape: {g.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SktgLqERWyef",
        "outputId": "70ec858f-bb68-45d4-a89c-6a3804e27ddd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense_1/kernel:0, shape: (3, 2)\n",
            "dense_1/bias:0, shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What Tapes Watch?\n",
        "\n",
        "The default behavior is to record all operations after accessing a <u>trainable</u> <u>`tf.Variable`</u>. The following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
      ],
      "metadata": {
        "id": "bowi6OBIZ5pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0 # a scalar tensor\n",
        "# Not a Variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "    print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPOkq1nuavh4",
        "outputId": "f1f10e32-43ed-494b-b1d1-d4a59f2463ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the variables being watched by the tape\n",
        "[var.name for var in tape.watched_variables()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40UV3E2Lbr95",
        "outputId": "b21a54d1-f13e-41c9-984d-2ff933a6f410"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`"
      ],
      "metadata": {
        "id": "EmvKv02eb2B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTj6weorb5fm",
        "outputId": "91c7e55a-3b16-4071-8d8d-7181b55d7d46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape."
      ],
      "metadata": {
        "id": "7Cp0AfRScGJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses two variables, but only connects the gradient for one of the variables\n",
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(x1)\n",
        "    y0 = tf.math.sin(x0)\n",
        "    y1 = tf.nn.softplus(x1)\n",
        "    y = y0 + y1\n",
        "    ys = tf.reduce_sum(y)\n",
        "\n",
        "# Since GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n",
        "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "585iG-WvcM3j",
        "outputId": "6ce8b55b-4221-4b81-cea5-89b2060bbe58"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dx0: None\n",
            "dy/dx1: 0.9999546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradients of Non-scalar Targets\n",
        "A gradient is fundamentally an operation on a scalar. Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
        "- The gradient of the sum of the targets, or equivalently\n",
        "- The sum of the gradients of each target.\n",
        "\n"
      ],
      "metadata": {
        "id": "24sCKSU7f0Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    y0 = x**2   # gradient = 2x\n",
        "    y1 = 1 / x  # gradient = -(1/(x^2))\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpbFvl1ngBtx",
        "outputId": "1ea92751-6d53-4d89-c7d4-27a1f137dc7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
      ],
      "metadata": {
        "id": "BcgSecMkhUcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x * [3., 4.]  # gradient = 3.0 + 4.0 since y = [3.0 * x, 4.0 * x]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqAei3ixhU3p",
        "outputId": "bdcea8f6-c37d-4d0d-acc0-2bc8c9e08575"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cases where `gradient` returns `None`\n",
        "When a target is not connected to a source, `gradient` will return `None`.\n"
      ],
      "metadata": {
        "id": "7PMIw4BsicX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = y * y   # z is not connected to x\n",
        "print(tape.gradient(z, x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYmU3TuUi6BF",
        "outputId": "1b6b983a-95dc-4a6d-f2f3-542b9399496e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Replaced a `Variable` with a `Tensor`\n",
        "The tape will automatically watch a `tf.Variable` but not a `tf.Tensor`. Hence, one common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`."
      ],
      "metadata": {
        "id": "ToSjQr8RjDy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y = x + 1\n",
        "\n",
        "    print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "    x = x + 1   # This should be `x.assign_add(1)`, and '1' is a scalar tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Mvv4CDjXQj",
        "outputId": "b1637e19-8b5e-4947-ddc7-8d03a7bfb9d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "EagerTensor : None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Did calculations outside of TensorFlow\n",
        "The tape can't record the gradient path if the calculation exits TensorFlow."
      ],
      "metadata": {
        "id": "TAOwBaccjo3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    x2 = x**2\n",
        "\n",
        "    # This step is calculated with NumPy rather than TensorFlow\n",
        "    y = np.mean(x2, axis=0)\n",
        "\n",
        "    # Like most ops, reduce_mean will cast the NumPy array\n",
        "    # to a constant tensor by using `tf.convert_to_tensor`.\n",
        "    y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lipialP6jr7W",
        "outputId": "cb3295fc-d0df-40a3-8163-c784ec7b5532"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Took Gradients Through an Integer or String\n",
        "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
        "\n",
        "Nobody expects `strings` to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`.\n"
      ],
      "metadata": {
        "id": "OJQTndiglPdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(10)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x * x\n",
        "\n",
        "print(g.gradient(y, x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nvdtsPxlca_",
        "outputId": "46c13020-7509-4bf7-c133-e718f20e3d1d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Took gradients through a stateful object\n",
        "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
        "\n",
        "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n",
        "\n",
        "A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:"
      ],
      "metadata": {
        "id": "yl6xYTBIlmD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Update x1 = x1 + x0.\n",
        "    x1.assign_add(x0)\n",
        "    # The tape starts recording from x1.\n",
        "    y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work. x0 is a state value that leads to x1\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttEKEOs2mBml",
        "outputId": "409be4b1-89aa-4032-a885-09cdad9714ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference\n",
        "- [Automatic Differentiation and Gradients](https://www.tensorflow.org/guide/autodiff)"
      ],
      "metadata": {
        "id": "T1rd5oe1fDfy"
      }
    }
  ]
}